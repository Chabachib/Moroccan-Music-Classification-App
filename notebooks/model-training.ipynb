{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_0OCzwVoycE"
   },
   "source": [
    "### Features Used in Training\n",
    "\n",
    "#### Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "\n",
    "1. **What is MFCC?**\n",
    "    - **MFCC** stands for Mel-Frequency Cepstral Coefficients. It is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a non-linear mel scale of frequency.\n",
    "\n",
    "2. **Why Use MFCC?**\n",
    "    - **Human Hearing**: The mel scale closely approximates the human ear's response to different frequencies. This makes MFCC particularly effective for tasks related to human speech and music analysis.\n",
    "    - **Feature Representation**: MFCCs capture the timbral texture of audio, which is crucial for distinguishing between different music genres. They encapsulate important frequency characteristics while reducing the dimensionality of the data.\n",
    "    - **Robustness**: MFCCs are less sensitive to noise and distortions compared to raw audio signals, making them more reliable for classification tasks.\n",
    "\n",
    "3. **Parameters Used in MFCC Calculation**:\n",
    "    - `num_mfcc=40`: Number of MFCC coefficients to extract. Typically, more coefficients capture more details but also increase computational complexity.\n",
    "    - `n_fft=2048`: Length of the FFT window. A higher value provides better frequency resolution.\n",
    "    - `hop_length=512`: Number of samples between successive frames. This controls the overlap between frames, balancing time resolution and computational load.\n",
    "    - `num_segment=10`: Number of segments to divide each audio file into. This helps in creating more training samples and capturing temporal variations within the audio.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "#### Sequential Model\n",
    "\n",
    "The architecture is a Sequential model, which means layers are stacked in a linear order. This simplicity ensures easy debugging and tuning.\n",
    "\n",
    "#### LSTM Layers\n",
    "\n",
    "1. **LSTM (Long Short-Term Memory) Layers**:\n",
    "    - **Why LSTM?**: LSTM networks are a type of recurrent neural network (RNN) capable of learning long-term dependencies. They are particularly well-suited for sequential data like audio signals.\n",
    "    - **Layer 1**: `LSTM(64, input_shape=(input_shape[0], input_shape[1]), return_sequences=True)`\n",
    "        - **64 units**: Number of LSTM units, which determines the dimensionality of the output space. This allows the network to capture complex temporal dependencies.\n",
    "        - **input_shape**: Specifies the shape of the input data. `input_shape[0]` represents the time steps (number of frames), and `input_shape[1]` represents the number of MFCC coefficients.\n",
    "        - **return_sequences=True**: Ensures the output of this layer is a sequence, which is necessary when stacking LSTM layers. It passes the entire sequence to the next LSTM layer, allowing deeper temporal feature extraction.\n",
    "    - **Layer 2**: `LSTM(64)`\n",
    "        - Another LSTM layer with 64 units to further capture temporal dependencies from the previous layer's output.\n",
    "\n",
    "#### Dense Layers\n",
    "\n",
    "1. **Dense Layer 1**: `Dense(64, activation='relu')`\n",
    "    - **64 units**: A fully connected layer with 64 neurons.\n",
    "    - **ReLU Activation**: The Rectified Linear Unit (ReLU) activation function introduces non-linearity, enabling the network to learn more complex patterns.\n",
    "\n",
    "2. **Output Layer**: `Dense(10, activation='softmax')`\n",
    "    - **10 units**: Corresponds to the number of music genres. Each unit represents the probability of the input belonging to one of the genres.\n",
    "    - **Softmax Activation**: Converts the output into a probability distribution, which is suitable for multi-class classification tasks.\n",
    "\n",
    "### Compilation and Training\n",
    "\n",
    "1. **Optimizer**: `Adam(lr=0.001)`\n",
    "    - **Adam**: An adaptive learning rate optimization algorithm that combines the advantages of two other extensions of stochastic gradient descent, AdaGrad and RMSProp.\n",
    "    - **Learning Rate**: Set to 0.001, which is a common choice that often works well as a starting point.\n",
    "\n",
    "2. **Loss Function**: `sparse_categorical_crossentropy`\n",
    "    - Suitable for multi-class classification with integer labels. It computes the cross-entropy loss between the true labels and predicted labels.\n",
    "\n",
    "3. **Metrics**: `['accuracy']`\n",
    "    - Accuracy is a straightforward metric to gauge the performance of the classification model.\n",
    "\n",
    "4. **Training**: `classifier.fit(X_train, y_train, validation_data=(X_val,y_val), batch_size=32, epochs=60, verbose=2)`\n",
    "    - **Batch Size**: 32, which balances computational efficiency and the ability to generalize.\n",
    "    - **Epochs**: 60, which is the number of times the learning algorithm will work through the entire training dataset. This helps the model learn the underlying patterns better.\n",
    "    - **Validation Data**: Used to monitor the modelâ€™s performance on unseen data and prevent overfitting.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The chosen model architecture leverages LSTM layers to capture temporal dependencies in audio features (MFCC), which are critical for music genre classification. The dense layers further process these temporal features to make accurate genre predictions. The use of MFCC as input features ensures that the model focuses on the most relevant audio characteristics while maintaining a manageable level of complexity. This combination of LSTM and dense layers, optimized with the Adam optimizer, provides a robust framework for music genre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6539,
     "status": "ok",
     "timestamp": 1716298693053,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "nbvO_0RDAQSl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from random import randint\n",
    "import math\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1716298695347,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "hyjXFKcNAYRR"
   },
   "outputs": [],
   "source": [
    "path = 'Add-Your-Path'\n",
    "genre_dict = {\"gnawa\":0,\"chaabi\":1,\"andalusian\":2, \"rai\":3, \"imazighn\":4, \"rap\":5, \"pop\":6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3169,
     "status": "ok",
     "timestamp": 1716297411211,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "IF8wNJT2U1CE",
    "outputId": "adb530f3-e2ac-4af1-8dbe-f595f6ccc69d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1716298699522,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "X4woQoOaAm8h"
   },
   "outputs": [],
   "source": [
    "def preprocess(dataset_path, num_mfcc=40, n_fft=2048, hop_length=512, num_segment=10):\n",
    "    data = {'audio_path':[], \"labels\": [], \"mfcc\": []}\n",
    "    sample_rate = 22050\n",
    "    samples_per_segment = int(sample_rate * 30 / num_segment)\n",
    "\n",
    "    for label_idx, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        if dirpath == dataset_path:\n",
    "            continue\n",
    "        dirname = os.path.basename(dirpath)\n",
    "        print(dirname)\n",
    "        lbl = genre_dict[dirname]\n",
    "        for f in sorted(filenames):\n",
    "            if not f.endswith('.wav'):\n",
    "                continue\n",
    "            if(f==\"jazz.00054.wav\"):\n",
    "            # As librosa only read files <1Mb\n",
    "              continue\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "\n",
    "            try:\n",
    "                y, sr = librosa.load(file_path, sr=sample_rate)\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "              for n in range(num_segment):\n",
    "                 start_sample = samples_per_segment * n\n",
    "                 end_sample = start_sample + samples_per_segment\n",
    "\n",
    "                 mfcc = librosa.feature.mfcc(y=y[start_sample:end_sample], sr=sr, n_mfcc=40, n_fft=2048, hop_length = 512)\n",
    "\n",
    "                 mfcc = mfcc.T\n",
    "                 if len(mfcc) == math.ceil(samples_per_segment / hop_length):\n",
    "                     data[\"audio_path\"].append(file_path)\n",
    "                     data[\"mfcc\"].append(mfcc.tolist())\n",
    "                     data[\"labels\"].append(lbl)\n",
    "            except:\n",
    "                print(file_path + str(lbl))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466941,
     "status": "ok",
     "timestamp": 1716299170811,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "ZbZeD8ieApMU",
    "outputId": "25ae8586-5a71-445c-cb37-1441528411ba"
   },
   "outputs": [],
   "source": [
    "data = preprocess(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1716299764950,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "_0i_WBNqAq5k",
    "outputId": "bf170156-5a38-4012-a2f3-adc544067950"
   },
   "outputs": [],
   "source": [
    "data[\"audio_path\"][0], data[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54914,
     "status": "ok",
     "timestamp": 1716299822305,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "ixq0vj_wAzHB"
   },
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_csv('moroccanMusic_extracted_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8485,
     "status": "ok",
     "timestamp": 1716299905031,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "N37mauUpA3N3"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/moroccanMusic_extracted_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 355643,
     "status": "ok",
     "timestamp": 1716300264227,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "Uc7KsGC1A8-M"
   },
   "outputs": [],
   "source": [
    "X = np.array(data[\"mfcc\"])\n",
    "X = np.array([ast.literal_eval(x) for x in X])\n",
    "X = X.astype('float32')\n",
    "y = np.array(data[\"labels\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1716300292196,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "pS92cC49Bl2L",
    "outputId": "ce6ae25d-1c70-4d5a-bc4f-25eb92743ffd"
   },
   "outputs": [],
   "source": [
    "y_train[0], X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1716300296965,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "vcUbVZLqCSCi",
    "outputId": "f36e1712-5524-48ee-9555-1237f706a942"
   },
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1716300303232,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "8jsrs3xoCT7e",
    "outputId": "141a5c95-5bee-4afe-9850-5ae883e92e9d"
   },
   "outputs": [],
   "source": [
    "input_shape= (X_train.shape[1],X_train.shape[2])\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1716301481174,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "ZhyL-vmuCVc2"
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(layers.LSTM(64, input_shape=(input_shape[0], input_shape[1]), return_sequences=True))\n",
    "classifier.add(layers.LSTM(64))\n",
    "classifier.add(layers.Dense(64, activation='relu'))\n",
    "classifier.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147064,
     "status": "ok",
     "timestamp": 1716301632947,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "iPR9jWe7CZA-",
    "outputId": "f41abda6-76fd-4af2-c447-a3e934d59bab"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "classifier.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.summary()\n",
    "classifier.fit(X_train, y_train, validation_data=(X_val,y_val), batch_size=32, epochs=60, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10536,
     "status": "ok",
     "timestamp": 1716301647797,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "bbCCJAccCa5-",
    "outputId": "66d12cbe-edcc-49f4-ed84-0907a5b119ca"
   },
   "outputs": [],
   "source": [
    "train_loss, train_acc  = classifier.evaluate(X_train, y_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1920,
     "status": "ok",
     "timestamp": 1716301649716,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "fmhPD9lwCdGT",
    "outputId": "a1c3d801-187d-4b74-d86e-4e9e8335b356"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc  = classifier.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1334,
     "status": "ok",
     "timestamp": 1716304199753,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "meFiwcQGVElo"
   },
   "outputs": [],
   "source": [
    "model = load_model('Add-Your-Path/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716301653700,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "MPQU06M9CfFv"
   },
   "outputs": [],
   "source": [
    "number_to_genre_dict = {0:\"gnawa\" , 1:\"chaabi\", 2:\"andalusian\", 3:\"rai\", 4:\"imazighn\", 5:\"rap\", 6:\"pop\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716301654769,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "PxoYdGiBCgyE"
   },
   "outputs": [],
   "source": [
    "def print_class_name(classes):\n",
    "    print(f'the predicted class is {number_to_genre_dict[statistics.mode(classes)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716301656265,
     "user": {
      "displayName": "Ayoub Benali",
      "userId": "12518229924079460677"
     },
     "user_tz": -60
    },
    "id": "gBLg3F6zCjN3"
   },
   "outputs": [],
   "source": [
    "def class_pred(classifier, file):\n",
    "    y, sr = librosa.load(file)\n",
    "    oneSong = []\n",
    "    for n in range(10):\n",
    "        start_sample = 22050*3  * n\n",
    "        end_sample = start_sample + 22050*3\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=y[start_sample:end_sample], sr=sr, n_mfcc=40, n_fft=2048, hop_length = 512)\n",
    "\n",
    "        mfcc = mfcc.T\n",
    "\n",
    "        if len(mfcc) == math.ceil( 22050*3 / 512 ):\n",
    "                oneSong.append(mfcc.tolist())\n",
    "\n",
    "    oneSong = np.array(oneSong, dtype=object)\n",
    "    oneSong = nsong = np.asarray(oneSong).astype('float32')\n",
    "    oneSong.shape\n",
    "\n",
    "    with open('output.txt', 'w') as file:\n",
    "      # Iterate through the nested list\n",
    "      for sublist in oneSong:\n",
    "          # Write each sublist as a line in the file\n",
    "          for item in sublist:\n",
    "              # Convert each numerical value to a string and join them with commas\n",
    "              line = ','.join(str(value) for value in item)\n",
    "              # Write the line to the file\n",
    "              file.write(line + '\\n')\n",
    "\n",
    "    prediction = classifier.predict(oneSong)\n",
    "    classes_x =np.argmax(prediction,axis=1)\n",
    "    return classes_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZD0B8eTK_3z"
   },
   "outputs": [],
   "source": [
    "classifier.save(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
